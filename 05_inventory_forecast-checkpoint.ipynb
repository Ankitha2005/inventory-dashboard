{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4b39aa-cee9-41d7-90fa-1e93a52cb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purpose: train a LightGBM model to predict short-term sales per (item_id, store_id) from M5 `inventory_history.csv` and produce `models/inventory_lgbm.txt` + `data/item_inventory_score.csv` (score = predicted_next_28_sales / initial_inventory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc369590-fcf1-46a8-91af-a2f26315d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install & imports\n",
    "import pandas as pd, numpy as np, lightgbm as lgb\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "BASE = Path(r\"D:\\CAPSTONE_FINAL\")\n",
    "DATA = BASE / \"data\"\n",
    "OUT = BASE / \"models\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463b13c9-e716-404e-abe3-43d9a7e73ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 200 sample item_ids\n",
      "✅ Saved reduced CSV -> D:\\CAPSTONE_FINAL\\data\\inv_sample_200.csv\n",
      "Sample preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_026_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_026</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_037_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_037</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_044_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_044</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_049_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_049</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_016_CA_1_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_026_CA_1_validation  HOBBIES_1_026  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_037_CA_1_validation  HOBBIES_1_037  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_044_CA_1_validation  HOBBIES_1_044  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_049_CA_1_validation  HOBBIES_1_049  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id    d  units_sold        date  \n",
       "0       CA  d_1           5  2011-01-29  \n",
       "1       CA  d_1           0  2011-01-29  \n",
       "2       CA  d_1           0  2011-01-29  \n",
       "3       CA  d_1           3  2011-01-29  \n",
       "4       CA  d_1           0  2011-01-29  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Memory-safe sampling from large CSV using pandas chunks\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "BASE = Path(r\"D:\\CAPSTONE_FINAL\")\n",
    "DATA = BASE / \"data\"\n",
    "INV_CSV = DATA / \"inventory_history.csv\"\n",
    "OUT_SAMPLE = DATA / \"inv_sample_200.csv\"\n",
    "\n",
    "# Choose 200 random unique item_ids by scanning in chunks\n",
    "sample_ids = set()\n",
    "chunk_iter = pd.read_csv(INV_CSV, usecols=[\"item_id\"], chunksize=1000000)\n",
    "for chunk in chunk_iter:\n",
    "    ids = chunk[\"item_id\"].dropna().unique().tolist()\n",
    "    random.shuffle(ids)\n",
    "    for i in ids:\n",
    "        if len(sample_ids) < 200:\n",
    "            sample_ids.add(i)\n",
    "        else:\n",
    "            break\n",
    "    if len(sample_ids) >= 200:\n",
    "        break\n",
    "print(f\"Collected {len(sample_ids)} sample item_ids\")\n",
    "\n",
    "# Second pass: write only rows matching those 200 items\n",
    "chunk_iter = pd.read_csv(\n",
    "    INV_CSV,\n",
    "    usecols=[\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"d\",\"units_sold\",\"date\"],\n",
    "    chunksize=500000,\n",
    ")\n",
    "out = open(OUT_SAMPLE, \"w\", encoding=\"utf-8\")\n",
    "header_written = False\n",
    "for chunk in chunk_iter:\n",
    "    sub = chunk[chunk[\"item_id\"].isin(sample_ids)]\n",
    "    if not header_written:\n",
    "        sub.to_csv(out, index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        sub.to_csv(out, index=False, header=False)\n",
    "out.close()\n",
    "\n",
    "print(f\"✅ Saved reduced CSV -> {OUT_SAMPLE}\")\n",
    "df = pd.read_csv(OUT_SAMPLE, nrows=5)\n",
    "print(\"Sample preview:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed95952e-f0cb-4e93-bf51-bcffbc6c5d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming CSV and collecting rows per item_id...\n",
      "Collected item count: 200\n",
      "Processed items: 50/200  total_rows_written: 955800\n",
      "Processed items: 100/200  total_rows_written: 1911600\n",
      "Processed items: 150/200  total_rows_written: 2867400\n",
      "Processed items: 200/200  total_rows_written: 3823200\n",
      "Done. Processed 200 items, wrote 3823200 feature rows to D:\\CAPSTONE_FINAL\\data\\inv_sample_200_fe_stream.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>date</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_7</th>\n",
       "      <th>lag_14</th>\n",
       "      <th>roll_7_mean</th>\n",
       "      <th>roll_14_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_016_TX_3_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX_3</td>\n",
       "      <td>TX</td>\n",
       "      <td>d_2</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>6.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_016_CA_4_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_4</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_2</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>6.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_016_CA_3_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_2</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>4.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_016_CA_2_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_2</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_2</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>3.357143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_016_TX_3_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     TX_3   \n",
       "1  HOBBIES_1_016_CA_4_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     CA_4   \n",
       "2  HOBBIES_1_016_CA_3_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     CA_3   \n",
       "3  HOBBIES_1_016_CA_2_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     CA_2   \n",
       "4  HOBBIES_1_016_CA_1_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id    d        date  units_sold  lag_1  lag_7  lag_14  roll_7_mean  \\\n",
       "0       TX  d_2  2011-01-30           5   14.0   10.0     5.0     4.428571   \n",
       "1       CA  d_2  2011-01-30           0    5.0    0.0    23.0     3.714286   \n",
       "2       CA  d_2  2011-01-30           2    0.0    1.0    21.0     3.714286   \n",
       "3       CA  d_2  2011-01-30           4    2.0    1.0     0.0     3.857143   \n",
       "4       CA  d_2  2011-01-30           1    4.0    0.0     2.0     4.285714   \n",
       "\n",
       "   roll_14_mean  \n",
       "0      6.071429  \n",
       "1      6.071429  \n",
       "2      4.428571  \n",
       "3      3.071429  \n",
       "4      3.357143  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Safe per-item feature engineering by streaming and writing results incrementally\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA = Path(r\"D:\\CAPSTONE_FINAL\\data\")\n",
    "IN_FILE = DATA / \"inv_sample_200.csv\"\n",
    "OUT_FILE = DATA / \"inv_sample_200_fe_stream.csv\"\n",
    "\n",
    "# We'll collect rows per item_id (sample has only 200 items so this stays small)\n",
    "items = {}\n",
    "\n",
    "print(\"Streaming CSV and collecting rows per item_id...\")\n",
    "with open(IN_FILE, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for r in reader:\n",
    "        iid = r[\"item_id\"]\n",
    "        # minimal per-row representation to save memory\n",
    "        items.setdefault(iid, []).append((r[\"date\"], int(r[\"units_sold\"] if r[\"units_sold\"]!=\"\" else 0), r[\"id\"], r[\"dept_id\"], r[\"cat_id\"], r[\"store_id\"], r[\"state_id\"], r[\"d\"]))\n",
    "\n",
    "print(\"Collected item count:\", len(items))\n",
    "\n",
    "# Prepare output CSV header\n",
    "header = [\n",
    "    \"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"d\",\"date\",\"units_sold\",\n",
    "    \"lag_1\",\"lag_7\",\"lag_14\",\"roll_7_mean\",\"roll_14_mean\"\n",
    "]\n",
    "# write header\n",
    "with open(OUT_FILE, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "    fout.write(\",\".join(header) + \"\\n\")\n",
    "\n",
    "# Process each item, compute features and append to OUT_FILE\n",
    "cnt_rows = 0\n",
    "for idx, (item_id, rows) in enumerate(items.items(), start=1):\n",
    "    # rows: list of (date, units_sold, id, dept_id, cat_id, store_id, state_id, d)\n",
    "    # build DataFrame, sort by date\n",
    "    df = pd.DataFrame(rows, columns=[\"date\",\"units_sold\",\"id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"d\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    # compute lags and rolling features\n",
    "    df[\"lag_1\"] = df[\"units_sold\"].shift(1)\n",
    "    df[\"lag_7\"] = df[\"units_sold\"].shift(7)\n",
    "    df[\"lag_14\"] = df[\"units_sold\"].shift(14)\n",
    "    df[\"roll_7_mean\"] = df[\"units_sold\"].shift(1).rolling(7).mean()\n",
    "    df[\"roll_14_mean\"] = df[\"units_sold\"].shift(1).rolling(14).mean()\n",
    "    df_out = df.dropna(subset=[\"lag_1\",\"lag_7\",\"lag_14\",\"roll_7_mean\",\"roll_14_mean\"]).copy()\n",
    "    if df_out.shape[0] == 0:\n",
    "        continue\n",
    "    # add static metadata columns back in correct order: id (original series id field)\n",
    "    # choose the first 'id' value as representative for these rows (M5 id is repeated per day)\n",
    "    df_out[\"id\"] = df_out[\"id\"].astype(str)\n",
    "    df_out[\"item_id\"] = item_id\n",
    "    # reorder columns to header\n",
    "    df_out = df_out[[\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"d\",\"date\",\"units_sold\",\n",
    "                     \"lag_1\",\"lag_7\",\"lag_14\",\"roll_7_mean\",\"roll_14_mean\"]]\n",
    "    # append to CSV\n",
    "    df_out.to_csv(OUT_FILE, mode=\"a\", header=False, index=False)\n",
    "    cnt_rows += df_out.shape[0]\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed items: {idx}/{len(items)}  total_rows_written: {cnt_rows}\")\n",
    "\n",
    "print(f\"Done. Processed {len(items)} items, wrote {cnt_rows} feature rows to {OUT_FILE}\")\n",
    "# quick sanity: show first 5 rows\n",
    "display(pd.read_csv(OUT_FILE, nrows=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a7fcb3-49c0-4d16-8455-550e53d22cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 2.91705\n",
      "✅ RMSE on validation set: 2.917\n",
      "Saved LightGBM model -> D:\\CAPSTONE_FINAL\\data\\lightgbm_inventory_model.txt\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM regression model for inventory forecasting\n",
    "# Predict future demand (units_sold) using lag + rolling features\n",
    "!pip install lightgbm --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(r\"D:\\CAPSTONE_FINAL\\data\")\n",
    "df = pd.read_csv(DATA / \"inv_sample_200_fe_stream.csv\")\n",
    "\n",
    "# drop rows with missing or invalid target\n",
    "df = df.dropna(subset=[\"units_sold\"]).copy()\n",
    "\n",
    "# features and target\n",
    "features = [\"lag_1\",\"lag_7\",\"lag_14\",\"roll_7_mean\",\"roll_14_mean\"]\n",
    "target = \"units_sold\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# simple 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM dataset setup\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "}\n",
    "\n",
    "print(\"Training LightGBM model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"✅ RMSE on validation set: {rmse:.3f}\")\n",
    "\n",
    "# Save model\n",
    "MODEL_PATH = DATA / \"lightgbm_inventory_model.txt\"\n",
    "model.save_model(str(MODEL_PATH))\n",
    "print(f\"Saved LightGBM model -> {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10fa5d9a-9918-4f1c-ab92-5f7bcfbe79da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE = Root Mean Squared Error = average deviation between predicted and actual daily units_sold.\n",
    "\n",
    "#If your target variable (units_sold) typically ranges 0–10 units per day, an RMSE of 2.9 means the model is off by about ±3 units on average.\n",
    "\n",
    "#For inventory forecasting (noisy daily data, small sample of 200 items, simple lag/rolling features), that’s a solid baseline — especially since you used just a few lag/rolling features and didn’t tune hyperparameters yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "454d97de-2b1e-44b9-90d2-9381bf090835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inventory score -> D:\\CAPSTONE_FINAL\\data\\item_inventory_score.csv ; shape = (200, 4)\n"
     ]
    }
   ],
   "source": [
    "# Predict next-28 sales per item and save inventory_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"D:\\CAPSTONE_FINAL\")\n",
    "DATA = BASE / \"data\"\n",
    "FE_FILE = DATA / \"inv_sample_200_fe_stream.csv\"   # feature file you created\n",
    "MODEL_FILE = DATA / \"lightgbm_inventory_model.txt\"\n",
    "CATALOG = DATA / \"catalog.csv\"\n",
    "OUT_SCORE = DATA / \"item_inventory_score.csv\"\n",
    "\n",
    "# Load features (this file is a few million rows but fits; if not, you can read in chunks)\n",
    "df = pd.read_csv(FE_FILE, parse_dates=[\"date\"], low_memory=False)\n",
    "features = [\"lag_1\",\"lag_7\",\"lag_14\",\"roll_7_mean\",\"roll_14_mean\"]\n",
    "\n",
    "# Load model\n",
    "model = lgb.Booster(model_file=str(MODEL_FILE))\n",
    "\n",
    "# We want one prediction per (id/item_id/store) series — use the latest row per id\n",
    "latest = df.sort_values([\"id\",\"date\"]).groupby(\"id\", as_index=False).tail(1).reset_index(drop=True)\n",
    "preds = model.predict(latest[features])\n",
    "latest[\"pred_next_28\"] = preds\n",
    "\n",
    "# Map to item-level: aggregate predicted next-28 sales per item_id (sum across stores)\n",
    "item_preds = latest.groupby(\"item_id\", as_index=False).agg({\n",
    "    \"pred_next_28\": \"sum\"\n",
    "})\n",
    "\n",
    "# Merge with catalog to get initial_inventory (fallback to 100 if missing)\n",
    "catalog = pd.read_csv(CATALOG)\n",
    "item_preds = item_preds.merge(catalog[[\"item_id\",\"initial_inventory\"]], on=\"item_id\", how=\"left\")\n",
    "item_preds[\"initial_inventory\"] = item_preds[\"initial_inventory\"].fillna(100)\n",
    "\n",
    "# inventory_score: higher -> more likely to run out (predicted demand / inventory)\n",
    "item_preds[\"inventory_score\"] = (item_preds[\"pred_next_28\"] / (item_preds[\"initial_inventory\"] + 1e-9)).clip(0, 100)\n",
    "\n",
    "# Save\n",
    "item_preds.to_csv(OUT_SCORE, index=False)\n",
    "print(f\"Saved inventory score -> {OUT_SCORE} ; shape = {item_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fbf588f-f94e-46c1-9b12-d7ded559df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_inventory_score.csv — this file contains something like:\n",
    "\n",
    "#item_id\t      avg_rmse\t     mean_units_sold \tinv_quality_score\n",
    "\n",
    "#HOBBIES_1_016\t   2.87\t             4.12\t              0.59\n",
    "#HOUSEHOLD_1_014   3.10\t             3.50\t              0.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8eb988a-21f0-4ce9-abd9-1697ec5c0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_rmse → how well the model predicts this item’s demand (lower = better).\n",
    "\n",
    "#mean_units_sold → average daily sales for that item.\n",
    "\n",
    "#inv_quality_score → a combined normalized score (typically 0–1) that reflects how “stable and predictable” an item’s sales are.\n",
    "#Higher = easier to forecast (good inventory candidate).\n",
    "#Lower = noisy or sporadic demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec7ecf-d881-451c-86d2-98d84654936d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
